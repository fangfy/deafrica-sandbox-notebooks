{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/datacube/storage/masking.py:4: DeprecationWarning: datacube.storage.masking has moved to datacube.utils.masking\n",
      "  category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datacube\n",
    "from datacube.storage import masking\n",
    "\n",
    "dc = datacube.Datacube(app='test_animation')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#coastal change\n",
    "#Saint-Louis, Senegal\n",
    "area_name = 'st_louis'\n",
    "query = {'lat': (15.9, 16.02), 'lon':(-16.55,-16.43),\n",
    "        'output_crs': 'EPSG:32628',\n",
    "        'resolution': (-30,30),\n",
    "        }\n",
    "rgb_bands = ['red','green','blue']\n",
    "band_index = 'mndwi'\n",
    "clear_frac = 0.95\n",
    "band_index_cmap = 'Blues'\n",
    "band_index_min, band_index_max = -0.6, 0.5\n",
    "sensors = [5, 7, 8]\n",
    "mask_cloud = False\n",
    "percentile_stretch = (0.01, 0.98)\n",
    "resample = '4M'\n",
    "interval = 500"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#urban expansion\n",
    "#Nairobi, Kenya\n",
    "area_name ='nairobi'\n",
    "query = {'lat': (-1.47, -1.22), 'lon':(36.78,37.03),\n",
    "        'output_crs': 'EPSG:32637', #'EPSG:32737',\n",
    "        'resolution': (-30,30),\n",
    "        }\n",
    "rgb_bands = ['red','green','blue']\n",
    "band_index = 'mndwi'\n",
    "clear_frac = 0.95\n",
    "band_index_cmap = 'brg_r'\n",
    "band_index_min, band_index_max = -0.6, 0.5\n",
    "sensors = [5,7,8]\n",
    "mask_cloud = False\n",
    "percentile_stretch = (0.01, 0.98)\n",
    "resample = '1Y'\n",
    "rolling = False\n",
    "interval = 1000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#illegal mining, forest clearing \n",
    "#Ankobra River, Ghana\n",
    "area_name ='ankobra_river'\n",
    "query = {'lat': (5.09, 5.14), 'lon':(-2.265,-2.215),\n",
    "        'output_crs': 'EPSG:32630',\n",
    "        'resolution': (-30,30),\n",
    "        }\n",
    "rgb_bands = ['red','green','blue']\n",
    "band_index = 'mndwi'\n",
    "clear_frac = 0.5\n",
    "band_index_cmap = 'brg_r'\n",
    "band_index_min, band_index_max = -0.6, 0.5\n",
    "sensors = [5,7,8]\n",
    "mask_cloud = False\n",
    "percentile_stretch = (0.01, 0.98)\n",
    "resample = '1Y'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Agriculture \n",
    "area_name ='burundi'\n",
    "query = {'lat': (-3.29, -3.16), 'lon':(29.29,29.42), #'lat': (-3.34, -3.16), 'lon':(29.25,29.43),\n",
    "        'output_crs': 'EPSG:32635', 'time':('2014-12-01', '2018-12-30'),\n",
    "        'resolution': (-30,30),\n",
    "        }\n",
    "rgb_bands = ['red','green','blue']\n",
    "band_index = 'ndvi'\n",
    "clear_frac = 0.95\n",
    "band_index_cmap = 'RdYlGn'\n",
    "band_index_min, band_index_max = 0.1, 0.7\n",
    "sensors = [5,7,8]\n",
    "mask_cloud = False\n",
    "percentile_stretch = (0.02, 0.98)\n",
    "resample = None #'3M'\n",
    "rolling = False\n",
    "interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near Cape Town South Africa\n",
    "\n",
    "area_name = 'south_africa_monthly'\n",
    "query = {'lat': (-33.72, -33.62), 'lon':(19.48, 19.60),\n",
    "        'output_crs': 'EPSG:32734',\n",
    "        'resolution': (-10,10),\n",
    "        }\n",
    "rgb_bands = ['red','green','blue']\n",
    "band_index = 'ndvi'\n",
    "clear_frac = 0.9\n",
    "band_index_cmap = 'RdYlGn'\n",
    "band_index_min, band_index_max = 0.1, 0.7\n",
    "sensors = ['s2']\n",
    "mask_cloud = False\n",
    "percentile_stretch = (0.01, 0.98)\n",
    "resample = '1M'\n",
    "rolling = False\n",
    "interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_name = 'mali_wetland'\n",
    "\n",
    "lat, lon = 15.323201, -3.798044\n",
    "lat, lon = 15.6, -3.8\n",
    "buffer_y = 0.2\n",
    "buffer_x = 0.25\n",
    "\n",
    "query = {'lat': (lat-buffer_y, lat+buffer_y), \n",
    "         'lon':(lon-buffer_x, lon+buffer_x),\n",
    "        'output_crs': 'EPSG:32630',\n",
    "        'resolution': (-10,10),\n",
    "        'cloud_cover': (0, 5),\n",
    "        }\n",
    "rgb_bands = ['swir_1','nir','green']\n",
    "band_index = None #'ndvi'\n",
    "clear_frac = 0.95\n",
    "band_index_cmap = 'RdYlGn'\n",
    "band_index_min, band_index_max = 0.1, 0.7\n",
    "sensors = ['s2']\n",
    "mask_cloud = False\n",
    "percentile_stretch = (0.02, 0.98)\n",
    "resample = None\n",
    "rolling = False\n",
    "interval = 800\n",
    "best_per_month = True"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "area_name = 'circles_in_dessert'\n",
    "\n",
    "y, x = 22.22, 28.7\n",
    "\n",
    "query = {'lat': (y-0.5, y+0.5), \n",
    "         'lon':(x-0.55, x+0.55),\n",
    "        'output_crs': 'EPSG:32734',\n",
    "        'resolution': (-10,10),\n",
    "        }\n",
    "rgb_bands = ['red','green','blue']\n",
    "band_index = 'ndmi'\n",
    "clear_frac = 0.9\n",
    "band_index_cmap = 'RdBu_r'\n",
    "band_index_min, band_index_max = 0.1, 0.7\n",
    "sensors = ['s2']\n",
    "mask_cloud = False\n",
    "percentile_stretch = (0.02, 0.98)\n",
    "resample = None\n",
    "rolling = False\n",
    "interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from skimage.morphology import disk, binary_dilation\n",
    "\n",
    "bad = []\n",
    "\n",
    "def load_and_combine(query, base_bands =['red','green','blue'], band_index = None, mask_cloud=True, best_per_month = True,\n",
    "                     dilation=0, clear_frac=0.8, sensors=[]):\n",
    "    bands = base_bands.copy()\n",
    "    if not band_index is None:\n",
    "        if band_index.lower() == 'mndwi':\n",
    "            if not 'green' in bands: bands += ['green']\n",
    "            if not 'swir_1' in bands: bands += ['swir_1']\n",
    "        if band_index.lower() == 'ndbi':\n",
    "            if not 'nir' in bands: bands += ['nir']\n",
    "            if not 'swir_1' in bands: bands += ['swir_1']\n",
    "        if band_index.lower() == 'ndvi':\n",
    "            if not 'nir' in bands: bands += ['nir']\n",
    "            if not 'red' in bands: bands += ['red']\n",
    "    \n",
    "    datasets =[]\n",
    "    for sensor in sensors:\n",
    "        if sensor == 's2':\n",
    "            product = 's2_l2a'\n",
    "            if not 'SCL' in bands: bands += [\"SCL\"]\n",
    "        else:\n",
    "            product = 'ls%s_usgs_sr_scene'%sensor\n",
    "        \n",
    "        \n",
    "        dss = dc.find_datasets(\n",
    "            product=product, measurements=bands,**query)\n",
    "        #print(\"Found %d datasets for Landsat %d.\"%(len(dss), sensor))\n",
    "        for bad_id in bad: dss = [dataset for dataset in dss if not bad_id in dataset.uris[0]]\n",
    "        print(\"Found %d good datasets for sensor %s.\"%(len(dss), sensor))\n",
    "        if len(dss)==0:continue\n",
    "\n",
    "        if (len(dss)>100 and (not 'time' in query)) or best_per_month:\n",
    "            ds_set =[]\n",
    "            time_slice = np.datetime64('2016-12')\n",
    "            while time_slice < np.datetime64('now'):\n",
    "                time_slice += np.timedelta64(1,'M')\n",
    "                #for time_slice in [('2018-01-01','2018-06-30'),('2018-07-01','2018-12-31'),('2019-01-01','2019-06-30'),('2019-07-01','2019-12-31')]: #,('2020-01-01','2020-12-31')]:\n",
    "                sub_query = query.copy()\n",
    "                sub_query['time'] = str(time_slice)\n",
    "                dss = dc.find_datasets(product=product, measurements=bands,**sub_query)\n",
    "                for bad_id in bad: dss = [dataset for dataset in dss if not bad_id in dataset.uris[0]]\n",
    "                print(f\"Found {len(dss)} good datasets for sensor {sensor} for {time_slice}\")\n",
    "                if len(dss)==0:continue\n",
    "                ds = dc.load(product=product, group_by='solar_day', datasets=dss, measurements=bands,**sub_query)\n",
    "                \n",
    "                if product =='s2_l2a':\n",
    "                    mask = good = ds.SCL.isin([2, 4,5,6,7,10,11])\n",
    "                else:\n",
    "                    pq = dc.load(product=product, group_by='solar_day', datasets=dss, measurements=['pixel_qa'],**sub_query)\n",
    "                    mask = masking.make_mask(pq.pixel_qa, cloud='no_cloud', cloud_shadow='no_cloud_shadow', nodata=False)\n",
    "                \n",
    "                if 'blue' in ds: mask = mask & (ds.blue<1000)\n",
    "                if dilation >0:\n",
    "                    mask = ~(~mask).groupby('time').apply(binary_dilation,selem=disk(10))\n",
    "                good_frac = mask.mean(['x','y'])\n",
    "                #print(good_frac.sel(time=(good_frac>=clear_frac)))\n",
    "                if best_per_month:\n",
    "                    good = good_frac == good_frac.max()\n",
    "                else:\n",
    "                    good = good_frac >= lear_frac\n",
    "                if good.sum()==0: continue\n",
    "                ds_clean = ds.isel(time=good)\n",
    "                if mask_cloud:\n",
    "                    ds_clean = ds_clean.where(mask.isel(time=good))\n",
    "                ds_clean = ds_clean.where(ds_clean>0,) \n",
    "                ds_clean['mask'] = mask.isel(time=good)\n",
    "                ds_set.append(ds_clean.sortby('time'))\n",
    "                print(\"Found %d clear observations for sensor %s.\"%(len(ds_clean.time), sensor))  \n",
    "    \n",
    "            ds_clean = xr.concat(ds_set, dim='time')\n",
    "                    \n",
    "        else:\n",
    "            ds = dc.load(product=product, group_by='solar_day', datasets=dss, measurements=bands,**query)\n",
    "            if product =='s2_l2a':\n",
    "                mask = good = ds.SCL.isin([2, 4,5,6,7,11])\n",
    "            else:\n",
    "                pq = dc.load(product=product, group_by='solar_day', datasets=dss, measurements=['pixel_qa'],**query)\n",
    "                mask = masking.make_mask(pq.pixel_qa, cloud='no_cloud', cloud_shadow='no_cloud_shadow', nodata=False)\n",
    "            if 'blue' in ds: mask = mask & (ds.blue<1000)\n",
    "            if dilation >0:\n",
    "                mask = ~(~mask).groupby('time').apply(binary_dilation,selem=disk(10))\n",
    "            good = mask.mean(['x','y']) >= clear_frac\n",
    "            if good.sum()==0: continue\n",
    "            ds_clean = ds.isel(time=good)\n",
    "            if mask_cloud:\n",
    "                    ds_clean = ds_clean.where(mask.isel(time=good))\n",
    "            ds_clean = ds_clean.where(ds_clean>0,) \n",
    "            ds_clean['mask'] = mask.isel(time=good)\n",
    "            print(\"Found %d clear observations for sensor %s.\"%(len(ds_clean.time), sensor))\n",
    "        \n",
    "        if not band_index is None:\n",
    "            if band_index.lower() == 'mndwi': \n",
    "                ds_clean[band_index] = (ds_clean.green-ds_clean.swir_1)/(ds_clean.green+ds_clean.swir_1)\n",
    "            if band_index.lower() == 'ndbi':\n",
    "                ds_clean[band_index] = (ds_clean.swir_1-ds_clean.nir)/(ds_clean.swir_1+ds_clean.nir)\n",
    "            if band_index.lower() == 'ndvi':\n",
    "                ds_clean[band_index] = (ds_clean.nir-ds_clean.red)/(ds_clean.nir+ds_clean.red)\n",
    "        #ds_clean[band_index] = ds_clean[band_index].where(ds_clean.mask)\n",
    "        #datasets = xr.concat([datasets, ds_clean], dim ='time')\n",
    "        datasets.append(ds_clean)\n",
    "    \n",
    "    if len(datasets)==1:\n",
    "        return datasets[0]\n",
    "    elif len(datasets)>1:\n",
    "        combined_ds = xr.concat(datasets, dim='time')\n",
    "        combined_ds = combined_ds.sortby('time')\n",
    "        return combined_ds\n",
    "    else: return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 197 good datasets for sensor s2.\n",
      "Found 4 good datasets for sensor s2 for 2017-01\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 2 good datasets for sensor s2 for 2017-02\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 2 good datasets for sensor s2 for 2017-03\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 4 good datasets for sensor s2 for 2017-04\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 2 good datasets for sensor s2 for 2017-05\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 3 good datasets for sensor s2 for 2017-06\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 3 good datasets for sensor s2 for 2017-07\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 4 good datasets for sensor s2 for 2017-08\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 7 good datasets for sensor s2 for 2017-09\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 5 good datasets for sensor s2 for 2017-10\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 7 good datasets for sensor s2 for 2017-11\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 4 good datasets for sensor s2 for 2017-12\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 3 good datasets for sensor s2 for 2018-01\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 6 good datasets for sensor s2 for 2018-02\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 8 good datasets for sensor s2 for 2018-03\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 10 good datasets for sensor s2 for 2018-04\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 1 good datasets for sensor s2 for 2018-05\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 5 good datasets for sensor s2 for 2018-06\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 4 good datasets for sensor s2 for 2018-07\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 3 good datasets for sensor s2 for 2018-08\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 4 good datasets for sensor s2 for 2018-09\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 3 good datasets for sensor s2 for 2018-10\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 4 good datasets for sensor s2 for 2018-11\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 6 good datasets for sensor s2 for 2018-12\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 6 good datasets for sensor s2 for 2019-01\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 6 good datasets for sensor s2 for 2019-02\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 6 good datasets for sensor s2 for 2019-03\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 6 good datasets for sensor s2 for 2019-04\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 2 good datasets for sensor s2 for 2019-05\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 1 good datasets for sensor s2 for 2019-06\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 6 good datasets for sensor s2 for 2019-07\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 4 good datasets for sensor s2 for 2019-08\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 6 good datasets for sensor s2 for 2019-09\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 4 good datasets for sensor s2 for 2019-10\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 8 good datasets for sensor s2 for 2019-11\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 8 good datasets for sensor s2 for 2019-12\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 1 good datasets for sensor s2 for 2020-01\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 10 good datasets for sensor s2 for 2020-02\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 6 good datasets for sensor s2 for 2020-03\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 6 good datasets for sensor s2 for 2020-04\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 7 good datasets for sensor s2 for 2020-05\n",
      "Found 1 clear observations for sensor s2.\n",
      "Found 0 good datasets for sensor s2 for 2020-06\n",
      "Found 0 good datasets for sensor s2 for 2020-07\n"
     ]
    }
   ],
   "source": [
    "ds = load_and_combine(query, base_bands = rgb_bands, band_index = band_index, clear_frac=clear_frac, best_per_month=best_per_month,\n",
    "                      mask_cloud=mask_cloud, sensors=sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2017-01-03T10:53:18.000000000', '2017-02-02T10:45:36.000000000',\n",
       "       '2017-03-04T10:55:24.000000000', '2017-04-23T10:54:45.000000000',\n",
       "       '2017-05-03T10:50:41.000000000', '2017-06-02T10:54:35.000000000',\n",
       "       '2017-07-27T10:46:37.000000000', '2017-08-01T10:45:59.000000000',\n",
       "       '2017-09-20T10:46:22.000000000', '2017-10-10T10:49:47.000000000',\n",
       "       '2017-11-04T10:44:39.000000000', '2017-12-29T10:51:29.000000000',\n",
       "       '2018-01-13T10:49:17.000000000', '2018-02-17T10:54:12.000000000',\n",
       "       '2018-03-09T10:51:36.000000000', '2018-04-13T10:44:43.000000000',\n",
       "       '2018-05-08T10:50:35.000000000', '2018-06-22T10:50:27.000000000',\n",
       "       '2018-07-07T10:56:30.000000000', '2018-08-16T10:44:19.000000000',\n",
       "       '2018-09-20T10:47:12.000000000', '2018-10-10T10:50:53.000000000',\n",
       "       '2018-11-04T10:55:45.000000000', '2018-12-04T10:56:37.000000000',\n",
       "       '2019-01-28T10:56:46.000000000', '2019-02-12T10:56:42.000000000',\n",
       "       '2019-03-09T10:57:09.000000000', '2019-04-18T10:59:48.000000000',\n",
       "       '2019-05-23T10:56:49.000000000', '2019-06-12T10:56:49.000000000',\n",
       "       '2019-07-17T10:56:55.000000000', '2019-08-01T10:56:50.000000000',\n",
       "       '2019-09-30T10:56:47.000000000', '2019-10-30T10:56:49.000000000',\n",
       "       '2019-11-14T10:56:45.000000000', '2019-12-24T10:56:41.000000000',\n",
       "       '2020-01-28T10:56:42.000000000', '2020-02-07T10:56:38.000000000',\n",
       "       '2020-03-13T10:56:44.000000000', '2020-04-22T10:56:40.000000000',\n",
       "       '2020-05-07T10:56:50.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a, b in zip(ds.time.values,ds.mask.groupby('time').mean().values):print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if area_name == 'burundi':\n",
    "#    good = ds[band_index].groupby('time').max().values >0.2\n",
    "#    ds = ds.isel(time=good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not best_per_month:\n",
    "    ds.resample(time='1M').last().dropna('time', how='all')[rgb_bands].to_array().plot.imshow(col='time',col_wrap=4, robust=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = ds[rgb_bands+[band_index]]\n",
    "if resample:\n",
    "    if not rolling:\n",
    "        combined_ds = ds.resample(time=resample).median().dropna('time', how='all')\n",
    "    else:\n",
    "        combined_ds = ds.resample(time=resample).median().rolling(time=3, center=True, min_periods=1).median().dropna('time', how='all')\n",
    "else:\n",
    "    combined_ds = ds#.resample(time='1M').last().dropna('time', how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds[rgb_bands].to_array().plot.imshow(col='time',col_wrap=4, robust=True);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mask = combined_ds.mask & (combined_ds.blue < 1000)\n",
    "combined_ds.blue.where(mask).isel(time=2).plot.imshow(robust=True);\n",
    "#combined_ds.blue.where(mask).groupby('time').max().values\n",
    "combined_ds.mask.groupby('time').mean().values, mask.groupby('time').mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if band_index:\n",
    "    combined_ds[band_index].plot.imshow(col='time',col_wrap=4,vmin=0.1, vmax=0.6, cmap=band_index_cmap, add_colorbar=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if area_name == 'st_louis':\n",
    "    combined_ds = combined_ds.isel(time=slice(28, len(combined_ds.time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"../Scripts\")\n",
    "#from deafrica_plotting import animated_timeseries, animated_doubletimeseries\n",
    "\n",
    "from DEAPlotting import animated_timeseries, animated_doubletimeseries\n",
    "\n",
    "\n",
    "if resample: date_type = 'M8[Y]'\n",
    "else: date_type = 'M8[M]'\n",
    "\n",
    "if band_index:\n",
    "    animated_doubletimeseries(combined_ds[rgb_bands], combined_ds[[band_index]], '%s_with_%s.gif'%(area_name, band_index), \n",
    "                              width_pixels=1500, interval=interval, \n",
    "                              bands1=rgb_bands, bands2=[band_index],                               \n",
    "                              percentile_stretch1 = percentile_stretch, percentile_stretch2 = percentile_stretch,\n",
    "                              vmin_2 = band_index_min, vmax_2 = band_index_max,\n",
    "                              image_proc_func1=None, image_proc_func2=None,\n",
    "                              title1=combined_ds.time.values.astype(date_type), title2=band_index.upper(), #combined_ds.time.values.astype('M8[Y]'),\n",
    "                              show_date1=False, show_date2=False, animation_options = {'repeat_delay': interval*2},\n",
    "                              annotation_kwargs1={}, annotation_kwargs2={},\n",
    "                              onebandplot_cbar1=True, onebandplot_cbar2=True,\n",
    "                              onebandplot_kwargs1={}, onebandplot_kwargs2={'cmap':band_index_cmap},\n",
    "                              shapefile_path1=None, shapefile_path2=None,\n",
    "                              shapefile_kwargs1={}, shapefile_kwargs2={},\n",
    "                              time_dim1 = 'time', x_dim1 = 'x', y_dim1 = 'y',\n",
    "                              time_dim2 = 'time', x_dim2 = 'x', y_dim2 = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DEAPlotting import animated_timeseries\n",
    "animated_timeseries(combined_ds[rgb_bands], '%s_color.gif'%(area_name), \n",
    "                    width_pixels=len(combined_ds.x), interval=interval, \n",
    "                    bands=rgb_bands,                           \n",
    "                    percentile_stretch = percentile_stretch, \n",
    "                    image_proc_func=None, \n",
    "                    title=combined_ds.time.values.astype(date_type), \n",
    "                    show_date=False, animation_options = {'repeat_delay': interval*2},\n",
    "                    annotation_kwargs={}, \n",
    "                    onebandplot_cbar=True, \n",
    "                    onebandplot_kwargs={}, \n",
    "                    shapefile_path=None,\n",
    "                    shapefile_kwargs={}, \n",
    "                    time_dim = 'time', x_dim = 'x', y_dim = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
